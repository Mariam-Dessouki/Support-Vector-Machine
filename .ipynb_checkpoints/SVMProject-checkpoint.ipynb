{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine Tutorial\n",
    "## Introduction\n",
    "A Support Vector Machine or SVM is a Machine Learning model used mainly in supervised learning that can perform classification and regression. Classification occurs when every input ie a datapoint is mapped to a discrete output (example spam/not spam or male/female). While regression is when an output of a datapoint is predicted generally according to some function. For example \n",
    "we can use this straight line to predict a company's sales from the amount of it spends on advertisement.\n",
    "<img src='regression.png'>\n",
    "\n",
    "As seen from the function, the higher the advertisement budget the higher the company's sales. This image was taken from codingsight.com\n",
    "\n",
    "SVM is a model that can perform both of these techniques.More details about SVM will follow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iris dataset\n",
    "The iris dataset has 3 types of irises that are determined according to 4 features which include sepal and petal length and width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "types of irises in the data set include  ['setosa', 'versicolor', 'virginica']\n",
      "the features of the irises include  ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n"
     ]
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "flower_type = iris['target']\n",
    "features = iris['data']\n",
    "print(\"types of irises in the data set include \",list(iris.target_names))\n",
    "print(\"the features of the irises include \", list(iris.feature_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "Splitting the data (features) for training and testing. 80% percent of the data is used for training, every datapoint in X_train with its correspending output in y_train are used to train/fit the model. The X_test,remaing 20% of data, is then used to check the accuracy of the model on data it has not seen before, this is done by comparing the output of the model on X_test with the actual output of X_test that is y_test.\n",
    "The test_size paramater is used to determine the proportion of the data used in testing, random_state parameter is the random number generator if it is not set to a constant the instance will be generated by numpy.random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, flower_type, test_size=0.2, random_state=15)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine\n",
    "The SVM method is used in classification and regression because of the many benefits it provides. SVM can be used in high dimension spaces, that is datasets with many features.It can also be used when the features are greater than the sample.SVM finds the best line/plane that can be used to classify the data. It chooses the one with the greatest distance between the line and the points on either side. An optimal hyperplane is shown in the figure below.\n",
    "<img src='optimal-hyperplane.png'>\n",
    "\n",
    "In the following code pipeline was used in order to apply a list of transforms to an estimator.The first transform is StandardScaler which standardizes the features and scales them to variance instead of actual values.This is extremely useful when a feature tends to have a higher value than another. For example, in the housing example the size of the house is measured in thousands of feet while the poverty ratio of the house's neighborhood is measured on a scale from 0-1 therefore even though these two features are highly important the size values will dominate the poverty ratio if there is no feature scaling.\n",
    "\n",
    "The second transform is the LinearSVC which classifies the data linearly. The C parameter shows how much we want to avoid misclassifying the data, if the C value is high the model would prioritize getting the datapoints correctly classified at the expense of the data margin. On the other hand if the C value is small the model would try to increase the margin at the expense of misclassifying datapoints. A high C value can overfit, while a small C value can underfit.\n",
    "The loss parameter defines the loss/cost function to be used where the hinge function is generally equal to L(y) = max(0, 1-t*y)\n",
    "where y is the score of the function and t is the target.\n",
    "\n",
    "After applying these transforms, the .fit function is going to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf = Pipeline((\n",
    "(\"scaler\", StandardScaler()),\n",
    "(\"linear_svc\", LinearSVC(C=1, loss=\"hinge\")),\n",
    "))\n",
    "svm_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "y_predict = svm_clf.predict(X_test)\n",
    "score = r2_score(y_test,y_predict)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import grid_search\n",
    "params = {'C': np.logspace(start =-5,stop =15,base =2)}\n",
    "lsvm =  LinearSVC(loss=\"hinge\",random_state = 0)\n",
    "grid = grid_search.GridSearchCV(lsvm, params)\n",
    "grid = grid.fit(X_train,y_train)\n",
    "best = grid.best_estimator_\n",
    "y_predictgs = best.predict(X_test)\n",
    "scoregs = r2_score(y_test,y_predictgs)\n",
    "print(\"score is {}\".format(scoregs))\n",
    "best.get_params()['C']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Continuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import r2_score\n",
    "polykernel = Pipeline((\n",
    "(\"scaler\", StandardScaler()),\n",
    "(\"svm_clf\", SVC(kernel=\"rbf\",gamma ='auto'))\n",
    "))\n",
    "polykernel.fit(X_train, y_train)\n",
    "y_predictpoly = polykernel.predict(X_test)\n",
    "scorepoly = r2_score(y_test,y_predictpoly)\n",
    "scorepoly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
